---
title: 机器学习笔记
date: 2020-08-02 17:28:59
tags:
---

在读吴军的《智能时代》和《数学之美》时常常被技术的力量给震撼到，尤其是其中现实的例子特别具有说服力，未来的发展或许由人工智能驱动。当然在深度方面，自学肯定不如专科博士生，不过未来人工智能或许就像现在的编程一样成为一种基础工具。今年的计划之一就是学习人工智能，因为离开学习太久，学习理论的东西不如从前。这一篇指南 [如何用3个月零基础入门「机器学习」](https://zhuanlan.zhihu.com/p/29704017) 提供的建议非常受用。目前在看 吴恩达的 《机器学习》视频，已完成监督学习部分，这儿做一个阶段性复习。

这个视频所涉及到的理论知识很少，不过跟着练习做完也发现已经可以解决很多问题了。首先是机器学习分为监督学习和非监督学习。监督学习就是已知训练数据集的结果，对未来的一些数据进行预测，比如数字识别，邮件分类，房价预测。非监督学习就是我们不知道结果，比如推荐系统中发现同时购买的商品。

## 线性回归

### 单一特征参数

线性回归大家应该都已经使用过，记得大学做物理实验的时候需要根据收集到的数据找到实验变量直接的线性关系，实验中的数据处理很多用的就是最小二乘法。假设收集到的数据已经打点在坐标系上，要求的一元线性回归方程形如 `y = ax + b`，我们需要保证这个直线离所有的点尽可能的近。

这个课程给的例子是已知房屋面积和价格的一组数据，我们需要找到它们之间的关系以便我们预测房价。我们可以把假设函数定义为

$$h_\theta(x) = \theta_0 + \theta_1x$$

为了评估我们的函数模型，我们引入损失函数（Cost Function）（其中的 $\hat{y_i}$ 是预测值）：

$$J(\theta_0, \theta_1)=\frac{1}{2m}\sum_{i = 1}^{n}(\hat{y_i} - y_i)^2 = \frac{1}{2m}\sum_{i = 1}^{n}(h_θ(x_i) - y_i)^2$$

我们希望这个损失函数结果越小越好（最小二乘法），也就是求它的最小值。我们可以先回顾一下一元函数的极值求法，之前讲求根运算时介绍过[牛顿迭代法](https://fedeoo.github.io/math-for-programmers/chapter1/sqrt.html)，我们的递归式

$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$

图形化的解释：

![牛顿迭代法](/gallery/site/newton-method.png) 

初始迭代的点为 $x_0$，之后根据斜率和 $f(x_0)$ 计算第二个点 $x_1$，一直到斜率趋于 0。斜率的正负决定了收敛方向，斜率的大小决定了收敛的速度。再来看下梯度下降，简单来说我们只需要将 $f(x_0)$ 替换为一个常量 $\alpha$ 比如 0.03。 比较而言，牛顿法具有很好的收敛性，而梯度下降要简单一些。下面是使用梯度下降找到极值的直观例子，就像沿着山坡最陡峭的方向往下走。

![梯度下降](/gallery/site/gradient-descent.png)

从图上可以看出，我们求得的极值是局部最优可能不是全局最优的，这跟我们的初始值有关，不过这个不是问题。多元函数使用的是偏导，对于上面的损失函数，我们的递归式就是形如：

$$\theta_0 := \theta_0 - \alpha \frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1) \\\\
\theta_1 := \theta_1 - \alpha \frac{\partial }{\partial \theta_1}J(\theta_1, \theta_1)$$

一直迭代下去直到偏导趋于 0，因为极值点偏导为 0。其中 $\alpha$ 的选择不能太大，太大可能导致无法收敛，太小的话迭代次数较多。注意上面的计算需要同时计算，对比下上面牛顿法的迭代式，左侧的 $\theta_0$ 和 $\theta_1$ 是第 $n + 1$ 的 $\theta$ 而 $\theta_0$ 和 $\theta_1$ 是第 n 次的 $\theta$。

接下来就是求偏导了:

$$\begin{aligned}
\frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1) &= \frac{\partial }{\partial \theta_j}\frac{1}{2}(h_θ(x_i) - y_i)^2 \\\\
  &= (h_θ(x_i) - y_i)\frac{\partial }{\partial \theta_j}(h_θ(x_i) - y_i) \\\\
  &= (h_θ(x_i) - y_i)\frac{\partial }{\partial \theta_j}h_θ(x_i)
\end{aligned}$$


对于 $\frac{\partial }{\partial \theta_j}h_θ(x_i)$，当 $\theta_j$ 为 $\theta_0$ 时值为 1，当 $\theta_j$ 为 $\theta_1$ 时值为 x。


### 多元线性回归

上面的例子只有一个特征参数，实际当中的特征参数会有很多，这儿就扩展一下。

$$h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ... + \theta_nx_n$$

其中 $x_0$ 始终为 1。我们又可以将上面的假设函数写成向量的形式：

$$h_\theta(x) = \left [ \begin{matrix} \theta_0 \\\\
 \theta_1 \\\\ \theta_2 \\\\ ... \\\\ \theta_n \end{matrix} \right ]
\left [ \begin{matrix} x_0 & x_1 & x_2 & ... & x_n \end{matrix} \right ] = \theta^TX$$

损失函数为：

$$J(\theta) = \frac{1}{2m}\sum_{i = 1}^{m}(h_θ(x^{(i)}) - y^{(i)})^2 = \frac{1}{2m}\sum_{i = 1}^{m}(\theta^Tx^{(i)} - y^{(i)})^2$$

其中 $x^{(i)}$ 和 $y^{(i)}$ 对应第 i 组数据。

偏导  

$$\frac{\partial }{\partial \theta_j}J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}(\theta^Tx^{(i)} - y^{(i)})x_{j}^{(i)}$$


上面的假设只是多元一次函数，其实假设函数式可以为任意多项式形式。以上面例子，我们可以把 $x_1^2$ 当成一个新的特征 $x_2$来看待。

$$h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_1^2 + ... + \theta_nx_n$$



### 特征缩放

上面房价的例子中，房屋的面积范围是 100 - 1000 这取决于实际数据，而我们通常选择的步长参数 $\alpha$ 是比较小的一个数。这种情况下为了减少迭代次数，我们可以把已有的数据映射到 [-1, 1] 这个范围内。 

$$x_i := \frac{x_i - u_i}{s_i}$$ 

其中 $u_i$ 为第 i 个特征的平均值，$s_i$ 为第 i 个特征的范围。


### Normal equation

梯度下降是求解 $\theta$ 的一种方式，我们要求解的 $\theta$ 满足 

$$X\theta = y$$

我们可以用 求解线性方程的方式求得 $\theta$，因为 $X$ 不一定是方阵，我们需要两边同时先乘以 $X^T$

$$X^TX\theta = X^Ty$$

然后两边同时乘以 $X^TX$ 的逆矩阵，有

$$\theta = (X^TX)^{-1}X^Ty$$

如果特征矩阵不可逆，可能是存在冗余特征或者是样本太少特征太多。

矩阵求逆的时间复杂度为 $O(n^3)$ 在特征数比较少（比如小于 1000）的时候比较有优势。


上面这些就是线性回归的内容，现在就可以使用它解决房价预测的问题了。

## 逻辑回归

还有一个常见问题是分类问题，比如判断肿瘤是恶性还是良性，垃圾邮件分类。这类问题明显不能使用线性回归解决，假设函数 $h_\theta(x)=\theta^TX$ 结果可以是任意值，而实际值 y 只可能为 0 或 1。所以，我们必须确保假设函数 $h_\theta(x)$ 满足 $0 \leq h_\theta(x) \leq 1$。我们引入 Sigmoid 函数 

$$g(z) = \frac{1}{1 + e^{-z}}$$

![Sigmoid](/gallery/site/ml/sigmoid.png)

假设函数为 

$$h_\theta(x)=g(\theta^TX)$$

假设函数表示的意义是 y = 1 的可能性。可以表示为

$$h_\theta(x) = P(y = 1|x;\theta) = 1 - P(y = 0|x;\theta)$$

一般来说，当 $h_\theta(x) \geq 0.5$ 时 y 为 1，当 $h_\theta(x) < 0.5$ 时 y 为 0。也就是当 $\theta^TX \geq 0$ 时 y 为 1，当 $\theta^TX < 0$ 时 y 为 0。如图所示，在我们对样本分类时，其实就是在找到一条线，将其划分为两部分，在直线下方的小于 0，在直线上方的 大于 0 。当然上面也已经提到过，我们的假设函数可以是任意多项式的函数，不一定是一条直线。

![Sigmoid](/gallery/site/ml/decision-boundary.png)


### 损失函数 

这儿不能使用线性规划的损失函数，因为得到的不是凸函数。下面是我们的损失函数
$$J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}Cost(h_θ(x^{(i)}), y^{(i)})$$
其中当 y = 0 时 $Cost(h_\theta(x), y) = -log(1 - h_\theta(x))$；当 y  = 1 时，$Cost(h_\theta(x), y) = -log(h_\theta(x))$。损失函数的效果是当实际值 y 与 计算值相同时，损失为 0，如果相反则趋于无穷大。因为 y 要么为 0 要么为 1，损失函数也可以写成
$$Cost(h_\theta(x), y) = -ylog(h_\theta(x)) - (1 - y)log(1 - h_\theta(x))$$ 

完整的损失函数为 

$$J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}\left [ -y^{(i)}log(h_\theta(x^{(i)})) - (1 - y^{(i)})log(1 - h_\theta(x^{(i)})) \right ]$$

对应的偏导

$$\frac{\partial }{\partial \theta_j}J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_{j}^{(i)}$$

看上去和线性回归的很相似，这儿不再证明。

最后，如果目标分类大于 2 种，我们只需要每次分出一种，把其它的看作一类即可。


## 正则化和过拟合问题

假设我们有一组数据，数据的分布如图所示，我们可以大概看出它应该是一条曲线，左侧的图表示我们在用直线来拟合这个函数，可以看出不太合适，我们称之为欠拟合。同样的道理，右侧我们使用的是一个高阶的函数来拟合，虽然曲线穿过了所有的点，但是我们知道这样的函数不具有普适性，我们称之为过拟合。

![Overfitting](/gallery/site/ml/overfitting.png)

对于过拟合来说，除了减少参数，还可以使用正则化的方式。简单来说就是，我们希望保留一些特征参数，但是又不希望它的权重太重。我们在损失函数中加入惩罚项，注意我们并不惩罚 $\theta_0$ 所以， j 是从 1 开始的。

$$J(\theta) = \frac{1}{2m}\sum_{i = 1}^{m}(h_θ(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j = 1}^{n}\theta_j^2$$

总之，我们需要求得损失函数的最小值，所以不难理解当 $\lambda$ 比较大时，$\theta$ 会非常而趋于 0。

正则化的线性回归递归式（当然 $\theta_0$ 不变）

$$\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i = 1}^{m}(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}$$


对于 Normal Equation 来说，变为

$$\theta = (X^TX + \lambda \cdot L)^{-1}X^Ty$$

其中 L 类似单位矩阵，只不过第一个位置的元素为 0 。

## 小结

目前来看，整个视频涉及到的数学知识并不多，只需要会求偏导和矩阵运算即可。学完这部分内容可以说对机器学习有一个大概的如认识。紧接着的神经网络和支持向量机内容太多，分开来写。